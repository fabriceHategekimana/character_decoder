```{python}
from module import TransformerDecoder
import pandas as pd
import torch.nn as nn
import torch.optim as optim
import torch
```


```{python}
from char_dataset import CharDataset 
shakespeare_dataset = CharDataset("shakespeare.txt", block_size=128)
```

```{python}
from torch.utils.data import DataLoader
dataloader = DataLoader(shakespeare_dataset, batch_size=32, shuffle=True)
```

# Training

```{python}
decoder = TransformerDecoder(vocab_size=shakespeare_dataset.get_vocab_size(),
                           d_model=128,
                           n_layers=12,
                           n_heads=8,
                           d_ff=100,
                           max_len=512,
                           dropout=0.1)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(decoder.parameters(), lr=0.001)

import torch.nn.functional as F

for epoch in range(1):
    for input_data, target_data in dataloader:
        optimizer.zero_grad() 
        output = decoder(input_data)
		output = F.softmax(output, dim=2)
		loss = criterion(output.view(-1, 65), target_data.view(-1))
		loss.backward() 
		optimizer.step()

```

