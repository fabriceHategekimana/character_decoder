{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaB4Nbpnk_Mj"
      },
      "source": [
        "from module import TransformerDecoder\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot_nXmR2j7OZ",
        "outputId": "d546043a-f236-4df8-d4ca-61078d251d14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olacQs_Rk_Ml"
      },
      "source": [
        "from char_dataset import CharDataset\n",
        "shakespeare_dataset = CharDataset(\"shakespeare.txt\", block_size=128)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCk9pVwrtOmb"
      },
      "source": [
        "from char_dataset import CharDataset\n",
        "shakespeare_dataset = CharDataset(\"demo.txt\", block_size=128)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Z_GIIrk_Mm"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(shakespeare_dataset, batch_size=32, shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZUwNtlTk_Mm"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6uAVX1bk_Mn"
      },
      "source": [
        "decoder = TransformerDecoder(vocab_size=shakespeare_dataset.get_vocab_size(),\n",
        "                           d_model=128,\n",
        "                           n_layers=12,\n",
        "                           n_heads=8,\n",
        "                           d_ff=100,\n",
        "                           max_len=512,\n",
        "                           dropout=0.1).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(decoder.parameters(), lr=0.001)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "for epoch in range(1):\n",
        "    for input_data, target_data in dataloader:\n",
        "        output = decoder(input_data.to(device))\n",
        "        output = F.softmax(output, dim=2)\n",
        "        loss = criterion(output.view(-1, output.size(2)).to(device), target_data.view(-1).to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6fGTPutlGiV",
        "outputId": "fc55a592-f635-4ed7-c2f5-fe6ddbc90eab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(3.7139, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.eval()\n",
        "with torch.no_grad():\n",
        "    context0 = \"O God, Oh my God\"*8\n",
        "    context1 = \"\"\"We are accounted poor citizens, the patricians good.\n",
        "What authority surfeits on would relieve us: if they\n",
        "would yield us but the\"\"\"\n",
        "    context = context0\n",
        "    tokenized_context = shakespeare_dataset.to_integer(context).to(device)\n",
        "    y = decoder.generate(tokenized_context, 10)\n",
        "    y = torch.argmax(y, dim=2)\n",
        "    print(y)\n",
        "    completion = shakespeare_dataset.to_string(y[0])\n",
        "    print(\"completion:\", completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "2iXgW3fY7fHd",
        "outputId": "30814bde-753e-495e-f25f-cbea27ad6aa3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8a625e814b97>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m would yield us but the\"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtokenized_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshakespeare_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/char_dataset.py\u001b[0m in \u001b[0;36mto_integer\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/char_dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'z'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehexh_qnpMvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.eval()\n",
        "\n",
        "num_tokens_to_generate = 10\n",
        "\n",
        "with torch.no_grad():\n",
        "    tok_emb = decoder.WTE(tokenized_context)\n",
        "    pos_emb = decoder.WPE(tok_emb)\n",
        "    x = decoder.dropout(tok_emb + pos_emb)\n",
        "\n",
        "    for block in decoder.Blocks:\n",
        "        x = block(x)\n",
        "\n",
        "    x = decoder.Final_LayerNorm(x)\n",
        "    generated_tokens = decoder.LM_Head(x)\n",
        "    generated_tokens = generated_tokens[:, -num_tokens_to_generate:]\n",
        "    print(generated_tokens.shape)\n",
        "decoder.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omPADzY5ltwW",
        "outputId": "109da939-9992-4d95-e5a8-3bf788bfcfb9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 47])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerDecoder(\n",
              "  (WTE): Embedding(47, 128)\n",
              "  (WPE): PositionalEncoding()\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (Blocks): ModuleList(\n",
              "    (0-11): 12 x TransformerBlock(\n",
              "      (CausalSelfAttn): CausalSelfAttention(\n",
              "        (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (out_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (LayerNorm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (MLP): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=100, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=100, out_features=128, bias=True)\n",
              "      )\n",
              "      (LayerNorm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (Final_LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (LM_Head): Linear(in_features=128, out_features=47, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_dataset.itos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLNF__2G_uw-",
        "outputId": "374c4c07-c0c2-4ed4-c01c-933fe937114f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'c',\n",
              " 1: '\\n',\n",
              " 2: 'b',\n",
              " 3: 'h',\n",
              " 4: 'd',\n",
              " 5: ' ',\n",
              " 6: 'f',\n",
              " 7: 'g',\n",
              " 8: 'e',\n",
              " 9: 'a'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h33zE4j_bvuC",
        "outputId": "7f482ce4-c993-4b83-cb15-bcabe65a9b7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quit()"
      ],
      "metadata": {
        "id": "t5VnTwwhqJhw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6]]])\n",
        "print(a.shape)\n",
        "res = a.sum(dim=2)\n",
        "print(res)\n",
        "print(res.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWBf2KzVpOE8",
        "outputId": "7a8c0ab0-e8e7-4093-a960-2e8857d1bb7d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 2])\n",
            "tensor([[ 2,  4,  6,  8, 10, 12]])\n",
            "torch.Size([1, 6])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "mon_noyau_python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}